// -*- c++ -*-
//
// sebastiaan van paasen
// (c) 1998-2023 all rights reserved

// code guard
#if !defined(pyre_cuda_memory_Pinned_icc)
#error this file contains implementation details for pyre::cuda::memory::Pinned
#else

// metamethods
// constructor
template <class T, bool isConst>
pyre::cuda::memory::Pinned<T, isConst>::Pinned(cell_count_type cells) :
    // the cuda {malloc} api is not RAII friendly, so we initialize with a
    // {nullptr} and replace it with the actual allocation
    _host_data { nullptr },
    _cells { cells }
{
    // grab a spot
    pointer cpu_spot = nullptr;
    // compute the memory footprint
    auto footprint = cells * sizeof(value_type);
    // allocate memory on the cpu
    auto status = cudaMallocHost(&cpu_spot, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while allocating " << footprint
              << " bytes of host memory: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
        // and bail
        throw std::bad_alloc();
    }

    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "allocated " << footprint << " bytes at " << (void *) cpu_spot
            << pyre::journal::endl(__HERE__);

    // zero out the memory
    status = cudaMemset(cpu_spot, 0, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while initializing " << footprint
              << " bytes of host memory for the offset field: " << description << " (" << status
              << ")" << pyre::journal::endl(__HERE__);
        // and bail
        throw std::runtime_error(description);
    }

    // grab a new spot
    _device_data = nullptr;
    // allocate memory on the gpu
    status = cudaMalloc(&_device_data, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while allocating " << footprint
              << " bytes of device memory: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
        // and bail
        throw std::bad_alloc();
    }

    // all went well, so let me know
    channel << "allocated " << footprint << " bytes at " << (void *) _device_data
            << pyre::journal::endl(__HERE__);

    // if all went well, make a deleter for CUDA allocated memory
    auto destructor = [footprint](auto ptr) {
        // attempt to free the block of cpu memory
        auto status = cudaFreeHost(ptr);
        // if something went wrong
        if (status != cudaSuccess) {
            // make a channel
            pyre::journal::error_t error("pyre.cuda");
            // complain
            error << "while deallocating " << footprint
                  << " bytes of host memory: " << pyre::journal::newline << cudaGetErrorName(status)
                  << " (" << status << ")" << pyre::journal::endl(__HERE__);
        }
        // all went well
        pyre::journal::debug_t channel("pyre.cuda.pinned_t");
        // so let me know
        channel << "deallocated " << footprint << " bytes starting at " << (void *) ptr
                << " on the host" << pyre::journal::endl(__HERE__);
        // all done
        return;
    };

    // replace the {nullptr} with the new block and register the deallocator
    _host_data.reset(cpu_spot, destructor);
}

template <class T, bool isConst>
pyre::cuda::memory::Pinned<T, isConst>::Pinned(handle_type host_handle, cell_count_type cells) :
    _host_data { host_handle },
    _cells { cells }
{}

// destructor
template <class T, bool isConst>
pyre::cuda::memory::Pinned<T, isConst>::~Pinned()
{
    // attempt to free the block of cpu memory
    auto status = cudaFree(_device_data);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while deallocating " << _device_data
              << " from device memory: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
    }
    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "deallocated the device memory " << pyre::journal::endl(__HERE__);
    // all done
    return;
}


// interface
// get the number of cells in the block
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::cells() const -> cell_count_type
{
    // easy
    return _cells;
}

// get the memory footprint of the block
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::bytes() const -> size_type
{
    // scale the number of cells by the cell size
    return cells() * sizeof(value_type);
}

// access to the data pointer
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::data() const -> pointer
{
    // return the raw data pointer
    return _host_data.get();
}

// get the shared pointer
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::handle() const -> handle_type
{
    // easy
    return _host_data;
}

// get the device pointer
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::device() const -> pointer
{
    // easy
    return _device_data;
}

// data synchronization host to device
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::synchronizeHostToDevice()
{
    // set cuda error
    cudaError_t status;

    // copy the pinned memory
    status = cudaMemcpy(device(), data(), cells() * sizeof(value_type), cudaMemcpyHostToDevice);

    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while sending " << cells() * sizeof(value_type)
              << " bytes to the device: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
    }

    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "send " << cells() * sizeof(value_type) << " bytes to the device"
            << pyre::journal::endl(__HERE__);

    // all done
    return;
}

template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::synchronizeHostToDevice(
    difference_type startOffset, cell_count_type copySize)
{ // set cuda error
    cudaError_t status;

    // if the request is out of bounds
    if (startOffset + copySize > cells()) {
        // make a channel
        pyre::journal::firewall_t channel("pyre.memory.bounds");
        // and complain
        channel << "out of bounds access:" << pyre::journal::newline << "  index "
                << startOffset + copySize << " must be less than " << cells()
                << pyre::journal::newline
                << "  in pyre::cuda::memory::pinned_t::synchronizeHostToDevice"
                << pyre::journal::endl(__HERE__);
        // unreachable, unless the user has marked this error as non-fatal
        // clamp {pos} to the last element in the block
        copySize = cells() - startOffset - 1;
    }

    // copy the pinned memory
    status = cudaMemcpy(
        device() + startOffset, data() + startOffset, copySize * sizeof(value_type),
        cudaMemcpyHostToDevice);

    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while sending " << copySize * sizeof(value_type)
              << " bytes to the device: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
    }

    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "send " << copySize * sizeof(value_type) << " bytes to the device"
            << pyre::journal::endl(__HERE__);

    // all done
    return;
}

// data synchronization device to host
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::synchronizeDeviceToHost()
{
    // set cuda error
    cudaError_t status;

    // copy the pinned memory
    status = cudaMemcpy(data(), device(), cells() * sizeof(value_type), cudaMemcpyDeviceToHost);

    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while receiving " << cells() * sizeof(value_type)
              << " bytes from the device: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
    }

    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "received " << cells() * sizeof(value_type) << " bytes from the device"
            << pyre::journal::endl(__HERE__);

    // all done
    return;
}

template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::synchronizeDeviceToHost(
    difference_type startOffset, cell_count_type copySize)
{ // set cuda error
    cudaError_t status;

    // if the request is out of bounds
    if (startOffset + copySize > cells()) {
        // make a channel
        pyre::journal::firewall_t channel("pyre.memory.bounds");
        // and complain
        channel << "out of bounds access:" << pyre::journal::newline << "  index "
                << startOffset + copySize << " must be less than " << cells()
                << pyre::journal::newline
                << "  in pyre::cuda::memory::pinned_t::synchronizeHostToDevice"
                << pyre::journal::endl(__HERE__);
        // unreachable, unless the user has marked this error as non-fatal
        // clamp {pos} to the last element in the block
        copySize = cells() - startOffset - 1;
    }

    // copy the pinned memory
    status = cudaMemcpy(
        data() + startOffset, device() + startOffset, copySize * sizeof(value_type),
        cudaMemcpyDeviceToHost);

    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("pyre.cuda");
        // complain
        error << "while receiving " << copySize * sizeof(value_type)
              << " bytes from the device: " << pyre::journal::newline << cudaGetErrorName(status)
              << " (" << status << ")" << pyre::journal::endl(__HERE__);
    }

    // all went well
    pyre::journal::debug_t channel("pyre.cuda.pinned_t");
    // so let me know
    channel << "received " << copySize * sizeof(value_type) << " bytes from the device"
            << pyre::journal::endl(__HERE__);

    // all done
    return;
}

// iterator support
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::begin() const -> pointer
{
    // the beginning of the block
    return data();
}

template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::end() const -> pointer
{
    // one past the last cell in the block
    return data() + cells();
}

// data access
template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::at(size_type pos) const -> reference
{
    // if the request is out of bounds
    if (pos >= cells()) {
        // make a channel
        pyre::journal::firewall_t channel("pyre.memory.bounds");
        // and complain
        channel << "out of bounds access:" << pyre::journal::newline << "  index " << pos
                << " must be less than " << cells() << pyre::journal::newline
                << "  in pyre::cuda::memory::pinned_t::operator[]" << pyre::journal::newline
                << "  with a block on the heap at " << data() << pyre::journal::endl(__HERE__);
        // unreachable, unless the user has marked this error as non-fatal
        // clamp {pos} to the last element in the block
        pos = cells() - 1;
    }

    // return a reference to the cell at {pos}
    return data()[pos];
}

template <class T, bool isConst>
auto
pyre::cuda::memory::Pinned<T, isConst>::operator[](size_type pos) const -> reference
{
    // return a reference to the cell at {pos}
    return data()[pos];
}

#endif

// end of file
